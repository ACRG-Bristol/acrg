{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for HBMCMC code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of modules in acrg_hbmcmc allow a convenient way to perform 'hierachical Bayesian MCMC'. \\\n",
    "That is, we want to infer emissions of a partiular species (and the influence from the domain boundary) using a Markov Chain Monte Carlo algorithm. The 'hierarchy' is that this depends on 'hyperparameters', which are generally the uncertainties in the non-latent parameters involved in the estimation (i.e. not those we are trying to derive – the emissions – but others that are necessary, e.g. the measurement error). \\\n",
    "For a work-in-progress introuction to inverse modelling see: https://www.overleaf.com/project/5f8d6217aeca1900019a84ce "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code within acrg_hbmcmc relies heavily on other code in the ACRG repository, mainly name.py in acrg_name. This code is generally used to read footprints, a priori emissions etc from the ACRG directory structure. \\\n",
    "Currently the MCMC estimation is completed using the pymc3 library (see https://docs.pymc.io/), a well-established statistical library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What the code does conceptually\n",
    "Rather than taking you through the whole code, we can just take a brief overview of the stages taken in the code. The overarching aim of the code is to estimate the emissions of some species in space over a particular period of time using measurements of that species. We assume that over the inversion period (the period of time we run the code over) the emissions are constant at a particular location.\\\n",
    "We estimate emissions in a limited region and, as well as the emissions, we also need consider the contribution to the measurement from background air – termed the boundary conditions. We also estimate these in the inversion process. To make things easier, we reparameterise our emissions so that we estimate by how much they are scaled up or down from some a priori value. What this means is that we adjust some initial best guess by a factor, which we limit to positive values as we are dealing only with emissions (and not sinks). The reason for doing this is that it is easier to do the statistics on a model where the numbers inferred are similar (i.e. generally all around a scaling factor of 1 rather than spanning the orders of magnitude seen by emissions) and that we can do stuff like force zero emissions over the oceans (i.e. nomatter how much we scale zero in our 'best guess', it will always be zero). In addition, we also estimate the errors in addition to the measurement error in our inverse method, and the uncertainty in this is propagated through to the emissions estimates. \\\n",
    "The code roughly follows these steps:\n",
    "1) Read the user defined inputs from a file (see next section)\\\n",
    "2) Reads in the required measurement data which will inform the inversion\\\n",
    "3) Read in the footprints, which map emissions to the measurements. For each grid cell of the disperion model output (usually NAME), there is the mole fraction contribution to the measurement assuming that the emissions are our 'best guess' or a priori emissions from the emissions file.\\\n",
    "4) Reduces the dimensionality of the emissions from the dispersion model resolution to a coarser resolution.\\\n",
    "5) Filter the data if needed (e.g. if you only want daytime measurements)\n",
    "6) Place the data into appropriate matrices for passing to an MCMC algorithm.\\\n",
    "7) Pass the data to an MCMC routine to estimate the emissions, errors and boundary conditions. This will prepare the various statistical models chosen to model the inference.\\\n",
    "8) Put the output into a netcdf file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to run the hbmcmc code is to copy the file hbmcmc_input_template.ini in acrg_hbmcmc/config/ to your run directory, and edit this code with your desired set up. \\\n",
    "After editing the config script, and in this case saving it as a file called 'hbmcmc_ch4_run.ini', you can run it from the command line (assuming that both are in the same directory) using\n",
    "```\n",
    "python run_hbmcmc.py -c hbmcmc_ch4_run.ini\n",
    "```\n",
    "The code generally explains the various inputs, but we will go through them in a bit more detail below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "; Configuration file for HBMCMC code\n",
    "; Required inputs are marked as such.\n",
    "; All other inputs are optional (defaults will be used)\n",
    "\n",
    "[INPUT.MEASUREMENTS]\n",
    "; Input values for extracting observations\n",
    "; species (str) - species name (see acrg_species_info.json for options) e.g. \"ch4\"\n",
    "; sites (list) - site codes as a list (see acrg_site_info.json for options) e.g. [\"MHD\"]\n",
    "; meas_period (list) - Time periods for measurements as a list (must match length of sites)\n",
    "; start_date (str) - Start of observations to extract (format YYYY-MM-DD)\n",
    "; end_date (str) - End of observations to extract (format YYYY-MM-DD) (non-inclusive)\n",
    "\n",
    "species = ''      ; (required)\n",
    "sites = []        ; (required)\n",
    "meas_period = []  ; (required)\n",
    "start_date = ''   ; (required - but can be specified on command line instead)\n",
    "end_date = ''     ; (required - but can be specified on command line instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**species** is a single string of the species you wish to do an inversion for, e.g. \"CH4\" or \"CFC-11\". As it says above, checkout out acrg_species_info.json for the various options currently available, or add your own if needed. \\\n",
    "**sites** is a list containing the sites you wish to use measurements from defined by their 3-letter code, e.g. [\"MHD\", \"TAC\"] for Mace Head and Tacolneston measurement sites. Again, as above, see acrg_site_info.json for options or add your own. \\\n",
    "**meas_period** is a list of the averaging you wish to apply to the measurements at the differnt sites. Often not much is gained by having many measurements made at really high frequency, especially as the footprints are rarely such high resolution. Instead we may wish to use a coarser frequency, e.g. every 6 hours, where all measurements made within a 6-hour period are averaged into a single measurement. A meas_period much be supplied in the list for each site, e.g. [\"6H\",\"6H\"]. \\\n",
    "**start_date** is when you want the inversion to begin, as a string (see above). \\\n",
    "**end_date** is when you want the inversion to end, as a string (see above). Note that the day is not included (e.g. \"2000-01-01\" would be until 23:59 on 1999-12-31) \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "; inlet (list/None) - Specific inlet height for the site (list - must match number of sites)\n",
    "; instrument (list/None) - Specific instrument for the site (list - must match number of sites)\n",
    "; obs_directory (str/None) - Directory containing the obs data (with site codes as subdirectories)\n",
    "\n",
    "inlet = None\n",
    "instrument = None\n",
    "obs_directory = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These inputs provide information about where the measurement data is read from, and to use measurements that use something other than the default set-up. These inputs can probably be ignored if you are just getting started. \\\n",
    "**inlet** is the height at which the air is sampled from. If set to None, the default will be used. For many sites, there is only one inlet anyway and so it will just default to this one height (e.g. MHD at 10m). Other sites have multiple inlets, e.g. TAC has inlets at 50m, 100m and 185m (NB. these heights are above ground level). TAC will default to 185m, but we may wish to use a different height, e.g. 100m, and so can specify this here. Note that you then have to specify this in a list of the same length as the *sites* input. So for MHD and TAC, we could write inlet=[None, \"100m\"] to use the 100m inlet at TAC and the defauls for MHD. \\\n",
    "**intrument** specified which instrument made the measurement. Many gases are measured at a site using multiple instruments. If this is the case, the defaul should reflect the best choice. Sometimes you might want to override this, e.g. if the default instrument was down for a long period. Again, if specifying this for one site you must have an entry for each site, e.g. to use the CRDS instrument at MHD and default at TAC you could use intrument=[\"CRDS\", None]. \\\n",
    "**obs_directory** specifies a directory other than the standard ACRG structure to read the measurement data from, specified as a string. The directory should have the structure /<site>/<obs_file> where the site and the obs file reflect what you would expect in the ACRG directory structure. This might be e.g. if you have some experimental data that is not suitable to be stored in the main ACRG obs file structure."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[INPUT.PRIORS]\n",
    "; Input values for extracting footprints, emissions and boundary conditions files (also uses values from INPUT.MEASUREMENTS)\n",
    "; domain (str) - Name of inversion spatial domain e.g. \"EUROPE\"\n",
    "; fpheight (dict/None) - Release height for footprints. e.g. fpheight = {\"TAC\":\"185m\"} (must match number of sites).\n",
    "; emissions_name (dict/None) - Name for specific emissions source. this should be a dictionary with {source_name: emissions_file_identifier} (e.g. {'anth':'co2-ff-mth'})\n",
    "; fp_directory (str/None) - Directory containing the footprints files (with domain name as subdirectories)\n",
    "; flux_directory (str/None) - Directory containing the emissions files (with domain name as subdirectories)\n",
    "\n",
    "domain = ''           ; (required) \n",
    "fpheight = None\n",
    "emissions_name = None\n",
    "fp_directory = None\n",
    "flux_directory = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of inputs is named \"INPUT.PRIORS\". This is a little misleading as it doesn't have anything to do with the priors... \\\n",
    "What this section does contain is the information about the region (defined by ACRG's preexisting regions) in which the inversion will take place and information about the footprints (sensitivities) that will be used to map the emissions to the measurements. \\\n",
    "**domain** is the model domain in which the inference will take place. Examples of a domain is \"EUROPE\" or \"SOUTHAFRICA\". You will probably have a good idea of which domain you're trying to infer before you start.\\\n",
    "**fpheight** is the disperion-model equivalent of the inlet height. The reason that this may be different to the true inlet height is that the topography in models does not always capture true topography (e.g. a site in a valley may be underground in the model). This has to be input as a dictionary and the height as magl, e.g. fpheight={\"MHD\":None, \"TAC\":\"100magl\"}.\n",
    "**emissions_name** specifies if you want to use a particular emissions file. An emissions file is is containst spatial information about the 'best guess' of what the emissions are in a particular domain (such as from an inventory). By default the inversion will use the file without a tag or named total and the most recent emissions file available. E.g., if you are running CH4 in EUROPE and the most recent emissions file is from 2010, it would default to something like ch4_EUROPE_2010.nc. But, if you want to use the emissions file ch4-oil-production_EUROPE_2010.nc, then set emissions_name=\"oil-production\".\n",
    "**fp_directory** just specifies the path to the footprints you want to use if not the defaults. The names and directory structure must mirror those in e.g. /shared/LPDM/fp_NAME/. You may need this if you are, e.g. experimenting with different footprints that are not suitable for sharing.\n",
    "**flux_directory** specifies the path to the emissions file you wish to use, if not the default ACRG path. The structure must mirror that of /shared/LPDM/emissions/."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[INPUT.BASIS_CASE]\n",
    "; Input values to extract the basis cases to use within the inversion for boundary conditions and emissions\n",
    "; bc_basis_case (str) - boundary conditions basis, defaults to \"NESW\" (looks for file format {bc_basis_case}_{domain}_*.nc)\n",
    "; fp_basis_case (str/None) - emissions bases:\n",
    "; - if specified, looks for file format {fp_basis_case}_{domain}_*.nc\n",
    "; - if None, creates basis function using quadtree algorithm and associated parameters\n",
    ";   - nbasis - Number of basis functions to use for quadtree derived basis function (rounded to %4)\n",
    "; basis_directory (str/None) - Directory containing the basis functions (with domain name as subdirectories)\n",
    "\n",
    "bc_basis_case = \"NESW\"\n",
    "\n",
    "fp_basis_case = None\n",
    "quadtree_basis = True\n",
    "nbasis = 100\n",
    "basis_directory = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis functions are the computational representation of the emissions. Most simply, this could be thought of as the underlying grid resolution of the inversion, but could also be something such a different countries or spatially distinct sectors. For the boundaries, this is how the boundary is broken down (e.g. as 4 sides, as a gradient etc.)\\\n",
    "**bc_basis_case** is how the inversion interprets the boundaries. Currently (unless this is out of date now) it can only handle \"NESW\", which means that the influence from the boundary at each cardinal direction is inferred.\\\n",
    "**fp_basis_case** is the basis function representation of the emissions. For example, if using the file 16x16_EUROPE_2012.nc, fp_basis_case=\"16x16\". Set to None (and quadtree_basis=True) to create the emissions basis functions on the fly.\\\n",
    "**quadtree_basis** set to True uses a quadtree algorithm to find a suitable basis function representation for emissions. Set to False if fp_basis_case is not None. The quadtree algorithm recursively divides the domain until the desired number of basis functions is reached, with the aim that regions with a higher contribution to the measurement (higher signal to noise) and more spatial variablity will have a finer spatial resolution than those that contribute little or are spatially uniform (e.g. far from the measurement site, oceans if not emitters, etc.) This is based on the a priori (\"best guess\") emissions multiplied by the average footprint for the inversion period, to give a mole fraction contributon at the dispersion-model resolution. Firstly, the domain is split into 4 new basis functions. The basis function that is most variable is then split into four new basis functions. Again, the basis function is split into 4 more until the desired number is achieved. There will always be a multiple of 3n+1 basis function.\\\n",
    "**nbasis** is the number of desired basis functions if using the quadtree algorithm. Note that if this is not a multiple of 3n+1 it will take th closest nubmer that works. There is no 'correct' number to use, but it will be harder to estimate a larger number of basis functions. However, too few may not represent reality well. This will require some experimentation on a case-by-case basis.\\\n",
    "**basis_directory** is the directory if not using the default ACRG location for the basis function. Again, this should mirror that of the ACRG file structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[MCMC.TYPE]\n",
    "; Which MCMC setup to use. This defines the function which will be called and the expected inputs.\n",
    "; Options include:\n",
    "; \"fixed_basis\"\n",
    "\n",
    "mcmc_type = \"fixed_basis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently this bit can't be changed, but in future there may be different options (e.g. trans-dimensional MCMC) all called from the same input script."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[MCMC.PDF]\n",
    "; Definitions of PDF shape and parameters for inputs\n",
    "; - xprior (dict) - emissions\n",
    "; - bcprior (dict) - boundary conditions\n",
    "; - sigprior (dict) - model error\n",
    "\n",
    "; Each of these inputs should be dictionary with the name of probability distribution and shape parameters.\n",
    "; See https://docs.pymc.io/api/distributions/continuous.html\n",
    "; Current options for the \"pdf\" parameter include:\n",
    "\n",
    "; - \"lognormal\" - Log-normal log-likelihood.\n",
    ";  - \"mu\" (float) - Location parameter\n",
    ";  - \"sd\" or \"sigma\" (float) - Standard deviation (> 0)\n",
    "; e.g. {\"pdf\":\"lognormal\", \"mu\":1, \"sd\":1}\n",
    "\n",
    "; - \"uniform\" - Continuous uniform log-likelihood.\n",
    ";  - \"lower\" (float) - Lower limit\n",
    ";  - \"upper\" (float) - Upper limit\n",
    "; e.g. {\"pdf\":\"uniform\", \"lower\":0.5, \"upper\":3}\n",
    "\n",
    "; - \"halfflat\" - Improper flat prior over the positive reals. (no additional parameters necessary)\n",
    "; e.g. {\"pdf\":\"halfflat\"}\n",
    "\n",
    "\n",
    "xprior = {\"pdf\":\"lognormal\", \"mu\":1, \"sd\":1}\n",
    "bcprior = {\"pdf\":\"lognormal\", \"mu\":0.004, \"sd\":0.02}\n",
    "sigprior = {\"pdf\":\"uniform\", \"lower\":0.5, \"upper\":3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explanation above is quite self explanatory in terms of what to input. If in doubt, then see https://docs.pymc.io/api/distributions/continuous.html."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[MCMC.BC_SPLIT]\n",
    "; Boundary conditions setup\n",
    "; - bc_freq - The period over which the baseline is estimated. e.g.\n",
    ";  - None - one scaling for the whole inversion\n",
    ";  - \"monthly\" - per calendar monthly\n",
    ";  - \"*D\" (e.g. \"30D\") - per number of days (e.g. 30 days)\n",
    "\n",
    "bc_freq = None\n",
    "sigma_freq = None\n",
    "sigma_per_site = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These inputs control the time period over which some variables are estimated. For example, if the inversion is over one year but we want to estimate the boundary conditions each month (rather than just once over the whole period). \n",
    "**bc_freq** is the frequency at which to estimate the boundary conditions. The way to input this is quite clearly explained above. Note that, as we're scaling, setting bc_freq=None does not mean that the boundary condition will have the same value for the whole inversion period, but it means that the whole period will be scaled by the same factor.\\\n",
    "**sigma_freq** is as bc_freq, but for the model uncertainty estimated in the process.\\\n",
    "**sigma_per_site** set to True estimates the model-measurement uncertainty for each site, else if set to False it does one estimate for all sites. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[MCMC.ITERATIONS]\n",
    "; Iteration parameters\n",
    "; nit (int) - Number of iterations for MCMC\n",
    "; burn (int) - Number of iterations to burn in MCMC\n",
    "; tune (int) - Number of iterations to use to tune step size\n",
    "\n",
    "nit = 2.5e5\n",
    "burn = 50000\n",
    "tune = 1.25e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is quite self explanatory. If the terms don't make sense then it's maybe best to do a little reading on MCMC algorithms. Some things to note, the number of iterations stored will be **nit** minus **burn**. That is, **nit** is the total iterations used for sampling. **tune** are iterations before sampling takes place, and so in total each MCMC chain will do **tune** + **nit** iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MCMC.NCHAIN]\n",
    "; Number of chains to run simultaneously. Must be >=2 to allow convergence to be checked.\n",
    "\n",
    "nchain = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number of chains that will be run (or how many independent MCMC estimates you will do). Only one chain will ever be stored, as one chain should contain all the info you need. If it doesn't then you have to up the number of iterations (and/or tune it better). The reason for running more than one chain is to try to understand whether your estimate is correct or not. In reality there is no way of knowing for certain that your esimate is correct, but we can tell if it wrong. In short, if we run an MCMC sampler twice (2 chains) and they give different estimates then we know we haven't run it for long enough as it hasn't converged. The more chains we run, the more sure we can be that it is giving the correct answer. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[MCMC.ADD_ERROR]\n",
    "; Add variability in averaging period to the measurement error\n",
    "\n",
    "averagingerror = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds an additional error to the measurement error. If, for example, our measurement instrument makes a measurement every hour, and we average this into 24H measurements, then averagingerror=True will add the variability in the measurements in this 24H period to the instrumental measurement error. The rationale behind this is that if the measurements are fairly similar, we can probably smooth them over 24H and represent them well in our emissions models. If they are very variable, then smoothing them over this window is likely not going to be represented well and thus leads to a larger error."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[MCMC.OUTPUT]\n",
    "; Details of where to write the output\n",
    "; outputpath (str) - directory to write output\n",
    "; outputname (str) - unique identifier for output/run name.\n",
    "\n",
    "outputpath = ''  ; (required)\n",
    "outputname = ''  ; (required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**outputpath** is simply the path to where you want your output to be stored.\\\n",
    "**outputname** is the tag you want to use to identify your output. E.g., if outputname=\"v1\", for the domain \"EUROPE\" and specied \"CH4\", with the run starting 2010-01-01, your output would be called \"CH4_EUROPE_v1_2010-01-01.nc\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ways to run the code\n",
    "You may wish to run the code for many different dates using identical inputs and not want to create a new config file each time. To just change the dates, e.g. to run the config file hbmcmc_ch4_run.ini for dates 2012-01-01 to 2013-01-01, you can run\n",
    "```\n",
    "python run_hbmcmc.py 2012-01-01 2013-01-01 -c hbmcmc_ch4_run.ini\n",
    "```\n",
    "In practice, you will want to run the script as a batch job, i.e. send it off to run on an HPC rather than run it on the command line. Below is an example shell script to run the hbmcmc code on Blue Pebble (BP1),\n",
    "```\n",
    "#!/bin/sh\n",
    "#PBS -l select=1:ncpus=4:mem=40GB\n",
    "#PBS -l walltime=12:00:00\n",
    "\n",
    "export OMP_NUM_THREADS=2\n",
    "export OPENBLAS_NUM_THREADS=2\n",
    "\n",
    "cd /<path>/<to>/<RunFolder>/\n",
    "\n",
    "python run_hbmcmc.py 2012-01-01 2013-01-01 -c hbmcmc_ch4_run.ini\n",
    "```\n",
    "You can run this on BP1, assuming the file is called ```run_PBS_ch4_EUROPE.sh``` using\n",
    "```\n",
    "qsub run_PBS_ch4_EUROPE.sh\n",
    "```\n",
    "If you wish to run this for many dates at the same time, you can use what's called an array job. The script below will run the config file for 10 years, with inversions over 1 year starting at the start of each year, starting in 2010. That is, the inversion will be for all of 2010, 2011, 2012...2019.\n",
    "```\n",
    "#!/bin/sh\n",
    "#PBS -l select=1:ncpus=4:mem=40GB\n",
    "#PBS -l walltime=12:00:00\n",
    "#PBS -J 1-10\n",
    "\n",
    "export OMP_NUM_THREADS=2\n",
    "export OPENBLAS_NUM_THREADS=2\n",
    "\n",
    "cd /<path>/<to>/<RunFolder>/\n",
    "\n",
    "Start='2010-01-01'\n",
    "Offset=$(expr $PBS_ARRAY_INDEX - 1)\n",
    "Start_Increment=\"$Offset years\"\n",
    "Increment=\"1 year\"\n",
    "Start_Date=$(date -I -d \"$Start + $Start_Increment\") || exit -1\n",
    "End_Date=$(date -I -d \"$Start_Date + $Increment\") || exit -1\n",
    "\n",
    "python run_hbmcmc.py $Start_Date $End_Date -c hbmcmc_ch4_run.ini\n",
    "```\n",
    "And again you would run this script using the qsub command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More detailed explanation of the script\n",
    "Here we can go through the main script in a little more detail. We will only focus on the top-level function ```fixedbasisMCMC``` in hbmcmc.py. This script generally calls all the relevant bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we read in the measurement data we want to use in our inversion. Many of the inputs will be familiar from the config script. We can denote the collection of measurements **y** for convenience.  \n",
    "```\n",
    "    data = getobs.get_obs(sites, species, start_date = start_date, end_date = end_date, \n",
    "                         average = meas_period, data_directory=obs_directory,\n",
    "                          keep_missing=False,inlet=inlet, instrument=instrument, max_level=max_level)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to read in our sensitivities, or the mapping from emissions to measurements. Note, as mentioned in the beginning, we actually want the senstivities multiplied by the a priori emissions to give a contribution in terms of mole fraction which can be scaled up and down. Let's denote this mapping **H** when in matrix form, such that when multiplied by sclaing **x**, **y=Hx**.\n",
    "```\n",
    "    fp_all = name.footprints_data_merge(data, domain=domain, calc_bc=True, \n",
    "                                        height=fpheight, \n",
    "                                        fp_directory = fp_directory,\n",
    "                                        bc_directory = bc_directory,\n",
    "                                        flux_directory = flux_directory,\n",
    "                                        emissions_name=emissions_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit just makes sure that there is some measurement data and footprints available – if not then there's no point in carrying on\n",
    "```\n",
    "    for site in sites:\n",
    "        for j in range(len(data[site])):\n",
    "            if len(data[site][j].mf) == 0:\n",
    "                print(\"No observations for %s to %s for %s\" % (start_date, end_date, site))\n",
    "    if sites[0] not in fp_all.keys():\n",
    "        print(\"No footprints for %s to %s\" % (start_date, end_date))\n",
    "        return\n",
    "    \n",
    "    print('Running for %s to %s' % (start_date, end_date))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit of code was a bit of hack – in short, there are different ways to quantify measurement error. Here we just say that if it contains both error metrics, just use the one called 'variability' as we can't use both. This is also explained in the comment.\n",
    "```\n",
    "    #If site contains measurement errors given as repeatability and variability, \n",
    "    #use variability to replace missing repeatability values, then drop variability\n",
    "    for site in sites:\n",
    "        if \"mf_variability\" in fp_all[site] and \"mf_repeatability\" in fp_all[site]:\n",
    "            fp_all[site][\"mf_repeatability\"][np.isnan(fp_all[site][\"mf_repeatability\"])] = \\\n",
    "                fp_all[site][\"mf_variability\"][np.logical_and(np.isfinite(fp_all[site][\"mf_variability\"]),np.isnan(fp_all[site][\"mf_repeatability\"]) )]\n",
    "            fp_all[site] = fp_all[site].drop_vars(\"mf_variability\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was explained in the config section, so I'll just copy and pase what was said there \"This adds an additional error to the measurement error. If, for example, our measurement instrument makes a measurement every hour, and we average this into 24H measurements, then averagingerror=True will add the variability in the measurements in this 24H period to the instrumental measurement error. The rationale behind this is that if the measurements are fairly similar, we can probably smooth them over 24H and represent them well in our emissions models. If they are very variable, then smoothing them over this window is likely not going to be represented well and thus leads to a larger error.\"\n",
    "```\n",
    "    #Add measurement variability in averaging period to measurement error\n",
    "    if averagingerror:\n",
    "        fp_all = setup.addaveragingerror(fp_all, sites, species, start_date, end_date,\n",
    "                                   meas_period, inlet=inlet, instrument=instrument,\n",
    "                                   obs_directory=obs_directory)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, this bit was explained fairly well in the config section, i.e. \"Basis functions are the computational representation of the emissions. Most simply, this could be thought of as the underlying grid resolution of the inversion, but could also be something such a different countries or spatially distinct sectors.\" Here we compute the user's choice of whether to use a premade basis function representation or make one on the fly using a quadtree algorithm, explained as \"The quadtree algorithm recursively divides the domain until the desired number of basis functions is reached, with the aim that regions with a higher contribution to the measurement (higher signal to noise) and more spatial variablity will have a finer spatial resolution than those that contribute little or are spatially uniform (e.g. far from the measurement site, oceans if not emitters, etc.) This is based on the a priori (\"best guess\") emissions multiplied by the average footprint for the inversion period, to give a mole fraction contributon at the dispersion-model resolution. Firstly, the domain is split into 4 new basis functions. The basis function that is most variable is then split into four new basis functions. Again, the basis function is split into 4 more until the desired number is achieved. There will always be a multiple of 3n+1 basis function.\" A basis function file will be temporarily saved in your run folder in a directory with some long name like Temp028ajfi.... and will be deleted at the end of the run (but also saved in the output). If for some reason the run fails after this point but before the end, then it will have to be removed manually.\n",
    "```\n",
    "    #Create basis function using quadtree algorithm if needed\n",
    "    if quadtree_basis:\n",
    "        if fp_basis_case != None:\n",
    "            print(\"Basis case %s supplied but quadtree_basis set to True\" % fp_basis_case)\n",
    "            print(\"Assuming you want to use %s \" % fp_basis_case)\n",
    "        else:\n",
    "            tempdir = basis.quadtreebasisfunction(emissions_name, fp_all, sites, \n",
    "                          start_date, domain, species, outputname,\n",
    "                          nbasis=nbasis)\n",
    "            fp_basis_case= \"quadtree\"+species+\"-\"+outputname\n",
    "            basis_directory = tempdir\n",
    "    else:\n",
    "        basis_directory = basis_directory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two lines reparameterise the sensitivities to emissions and the boundaries to the user's choice of basis functions\n",
    "```\n",
    "    fp_data = name.fp_sensitivity(fp_all, domain=domain, basis_case=fp_basis_case,basis_directory=basis_directory)\n",
    "    fp_data = name.bc_sensitivity(fp_data, domain=domain,basis_case=bc_basis_case)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you may wish to filter data (e.g. to remove local pollution event or only take measurements from certain times in the day.  The lines below do this if required\n",
    "```\n",
    "    #apply named filters to the data\n",
    "    fp_data = name.filtering(fp_data, filters)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another hacky bit so that we have the domain saved into the ```fp_data``` dataset\n",
    "```\n",
    "    for si, site in enumerate(sites):     \n",
    "        fp_data[site].attrs['Domain']=domain\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few lines turns everything we have read in into nice matrices that can be used in a statistical model. Our aim is to use the model **y=Hx+e** to estimate **x**, the scaling of emissions/boundary conditions, and also estimate **e**, the model-measurement error distribution, using our known measurements **y** and mapping **H**.\\\n",
    "Ultimately, this creates a hierarchical probabilistic model\\\n",
    "$p(x,\\sigma \\mid y) \\propto p(y\\mid x)p(x\\mid\\sigma)p(\\sigma)$\\\n",
    "where $\\sigma$ is the standard deviation(s) of the model-measurement error in addition to the known measurement error. The 'hierarchy' comes from the fact that the inference of **x** relies on $\\sigma$.\\\n",
    "It's easier to split our matrix **H** into two separate matrices, one for the emissions and one for the boundary condtions, and this is what we do below.\n",
    "```\n",
    "#Get inputs ready\n",
    "error = np.zeros(0)\n",
    "Hbc = np.zeros(0)\n",
    "Hx = np.zeros(0)\n",
    "Y = np.zeros(0)\n",
    "siteindicator = np.zeros(0)\n",
    "for si, site in enumerate(sites):\n",
    "    if 'mf_repeatability' in fp_data[site]:           \n",
    "        error = np.concatenate((error, fp_data[site].mf_repeatability.values))\n",
    "    if 'mf_variability' in fp_data[site]:\n",
    "        error = np.concatenate((error, fp_data[site].mf_variability.values))\n",
    "\n",
    "    Y = np.concatenate((Y,fp_data[site].mf.values)) \n",
    "    siteindicator = np.concatenate((siteindicator, np.ones_like(fp_data[site].mf.values)*si))\n",
    "    if si == 0:\n",
    "        Ytime=fp_data[site].time.values\n",
    "    else:\n",
    "        Ytime = np.concatenate((Ytime,fp_data[site].time.values ))\n",
    "\n",
    "    if bc_freq == \"monthly\":\n",
    "        Hmbc = setup.monthly_bcs(start_date, end_date, site, fp_data)\n",
    "    elif bc_freq == None:\n",
    "        Hmbc = fp_data[site].H_bc.values\n",
    "    else:\n",
    "        Hmbc = setup.create_bc_sensitivity(start_date, end_date, site, fp_data, bc_freq)\n",
    "\n",
    "    if si == 0:\n",
    "        Hbc = np.copy(Hmbc) #fp_data[site].H_bc.values \n",
    "        Hx = fp_data[site].H.values\n",
    "    else:\n",
    "        Hbc = np.hstack((Hbc, Hmbc))\n",
    "        Hx = np.hstack((Hx, fp_data[site].H.values))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line is just to help set up the $\\sigma$ term so that they can be distributed in time and/or by site if needed (see explanation in the config file).\n",
    "```\n",
    "sigma_freq_index = setup.sigma_freq_indicies(Ytime, sigma_freq)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the magic happens. Everything we have read in so far, an our description of the probabilistic mode, are passed to Pymc3 - a statistical library that perform MCMC to estimate what we wish to estimate. The emissions and boundary conditions are inferred using a method called NUTS (https://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf) which is really efficient. The hyperparameters (i.e. model error) uses a Slice sampler (DOI: 10.1214/aos/1056562461). The reason for using a different sampler was originally because a slice sampler is faster, and in general you don't need as many iterations for the hyperparameter (unless you have lots of them) and so I just used a less efficient but faster one.\\\n",
    "The inputs to the ```fixedbasisMCMC``` function takes the three arguments\n",
    "```\n",
    "xprior={\"pdf\":\"lognormal\", \"mu\":1, \"sd\":1},\n",
    "bcprior={\"pdf\":\"lognormal\", \"mu\":0.004, \"sd\":0.02},\n",
    "sigprior={\"pdf\":\"uniform\", \"lower\":0.5, \"upper\":3},\n",
    "```\n",
    "These are the prior probabilities for the emissions, boundary conditions and hyperparameters. The function ```inferpymc3``` in inversion_pymc3.py gives a bit more detail on these, as done the description in the config file, but in short the first bit is the name of the PDF (as per pymc3) and then follow the parameters that describe that distribution. Note that **these are not one size fits all distributions**. These are just some value I was using at the time of putting this together, i.e. for some VSLS in South Africa. There are many to choose from and you should really spend some time thinking about what best described your case. E.g. a lognormal(1,1) distribution is fairly uninformative, i.e. there is a 95% probability that the true value is between 0.38-19.3 times your a priori distribution. Emissions can only be positive, with the most probably value being that of the emissions value. The mean, however, is ~4.5x the a priori value, so be careful with your metrics. Other positively bounded distributions could be better suited, such as the truncated normal, but again this distribution has sharp changes which may be unrealistic and/or cause problems when sampling. The point is, don't just randomly pick something - think about it and think how this may affect the metrics you might be quoting in a piece of work, e.g. if using a LN(1,1) and your data provides no info, and you report on the posterior mean, then of course your posterior mean will be larger than your a priori emissions.\n",
    "```\n",
    "#Run Pymc3 inversion\n",
    "xouts, bcouts, sigouts, Ytrace, convergence, step1, step2 = mcmc.inferpymc3(Hx, Hbc, Y, error, siteindicator, sigma_freq_index,\n",
    "       xprior,bcprior, sigprior,nit, burn, tune, nchain, sigma_per_site, verbose=verbose)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part takes the output from the MCMC code and makes some nice metrics, such as country totals, and saves everything to a netcdf file (see config description). Hopefully the variables names are all quite self-explanatory, and there should be everything you need in there to reproduce the results. You can plot the data and things like that using ```hbmcmc_post_process.py```. \n",
    "```\n",
    "#Process and save inversion output\n",
    "mcmc.inferpymc3_postprocessouts(xouts,bcouts, sigouts, convergence, \n",
    "                       Hx, Hbc, Y, error, Ytrace,\n",
    "                       step1, step2, \n",
    "                       xprior, bcprior, sigprior,Ytime, siteindicator, sigma_freq_index, fp_data,\n",
    "                       emissions_name, domain, species, sites,\n",
    "                       start_date, end_date, outputname, outputpath,\n",
    "                       basis_directory, country_file, country_unit_prefix,flux_directory)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit just remove the temporary directory containing the quadtree basis functions if used.\n",
    "```\n",
    "if quadtree_basis is True:\n",
    "    # remove the temporary basis function directory\n",
    "    shutil.rmtree(tempdir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you see this then it should mean that it ran through successfully without errors (although this doesn't necessarily mean that the results are any good!).\n",
    "```\n",
    "print(\"All done\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
